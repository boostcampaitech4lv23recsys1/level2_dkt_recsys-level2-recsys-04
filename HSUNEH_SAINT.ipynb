{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    device = torch.device(\"cuda\")\n",
    "    MAX_SEQ = 100\n",
    "    EMBED_DIMS = 512\n",
    "    ENC_HEADS = DEC_HEADS = 8\n",
    "    NUM_ENCODER = NUM_DECODER = 4\n",
    "    BATCH_SIZE = 32\n",
    "    TRAIN_FILE = \"/opt/ml/input/data/train_data.csv\"\n",
    "    TEST_FILE = \"/opt/ml/input/data/test_data.csv\"\n",
    "    TOTAL_EXE = 13523\n",
    "    TOTAL_CAT = 10000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.데이터 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존 SAINT : train의 Column 종류\n",
    "\n",
    "row_id, timestamp, user_id, content_id, content_type_id,\n",
    "\n",
    "task_container_id, user_answer, answered_correctly, prior_question_elapsed_time, prior_question_had_explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리 DKT\n",
    "\n",
    "userID,assessmentItemID,testId,answerCode,Timestamp,KnowledgeTag\n",
    "\n",
    "공통적으로 User id, timestamp 있고\n",
    "\n",
    "콘텐츠적으로 나누면 우리 dkt 는 3개정도 분류될거 같고,\n",
    "\n",
    "saint+이기 때문에 time 에 관해서 더 프로세싱하면 될거 같다.(elapsed, Lagged time 은 데이터상으로 구별하기 힘들다.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "'timestamp': 'int64',  -> Timestamp\n",
    "\n",
    "'user_id': 'int32',  -> userID\n",
    "\n",
    "'content_id': 'int16' -> assessmentItemID\n",
    "\n",
    "'answered_correctly': 'int8',   -> answerCode\n",
    "\n",
    "\n",
    "\n",
    "\"content_type_id\": \"int8\", -> 0으로 생성\n",
    "\n",
    "\n",
    "\n",
    "\"prior_question_elapsed_time\": \"float32\", -> Timestamp 변형\n",
    "\n",
    "\"task_container_id\": \"int16\" -> 누적 개수 (새로 생성 필요)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading csv.....\n",
      "shape of (train) dataframe : (2525956, 6)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mshape of (train) dataframe :\u001b[39m\u001b[39m\"\u001b[39m, train_df\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     12\u001b[0m train_df[\u001b[39m'\u001b[39m\u001b[39mgrade\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m (train_df[\u001b[39m'\u001b[39m\u001b[39massessmentItemID\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mstr[\u001b[39m2\u001b[39m])\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mint\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m train_df[\u001b[39m'\u001b[39m\u001b[39mtestId\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m (train_df[\u001b[39m'\u001b[39;49m\u001b[39massessmentItemID\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mstr[\u001b[39m2\u001b[39;49m] \u001b[39m+\u001b[39m train_df[\u001b[39m'\u001b[39m\u001b[39massessmentItemID\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mstr[\u001b[39m4\u001b[39m:\u001b[39m7\u001b[39m])\n\u001b[1;32m     14\u001b[0m train_df[\u001b[39m\"\u001b[39m\u001b[39mTimestamp\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(train_df[\u001b[39m\"\u001b[39m\u001b[39mTimestamp\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     16\u001b[0m ItemID2idx \u001b[39m=\u001b[39m {v:k \u001b[39mfor\u001b[39;00m k,v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_df[\u001b[39m'\u001b[39m\u001b[39massessmentItemID\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique())} \u001b[39m# 9453개\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/strings/accessor.py:221\u001b[0m, in \u001b[0;36mStringMethods.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[0;32m--> 221\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data\u001b[39m.\u001b[39;49marray\u001b[39m.\u001b[39;49m_str_getitem(key)\n\u001b[1;32m    222\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_result(result)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/strings/base.py:29\u001b[0m, in \u001b[0;36mBaseStringArrayMethods._str_getitem\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_str_slice(start\u001b[39m=\u001b[39mkey\u001b[39m.\u001b[39mstart, stop\u001b[39m=\u001b[39mkey\u001b[39m.\u001b[39mstop, step\u001b[39m=\u001b[39mkey\u001b[39m.\u001b[39mstep)\n\u001b[1;32m     28\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 29\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_str_get(key)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/strings/object_array.py:270\u001b[0m, in \u001b[0;36mObjectStringArrayMixin._str_get\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[39mreturn\u001b[39;00m x[i]\n\u001b[1;32m    268\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_str_na_value\n\u001b[0;32m--> 270\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_str_map(f)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/strings/object_array.py:61\u001b[0m, in \u001b[0;36mObjectStringArrayMixin._str_map\u001b[0;34m(self, f, na_value, dtype)\u001b[0m\n\u001b[1;32m     59\u001b[0m convert \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mall(mask)\n\u001b[1;32m     60\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m     result \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer_mask(arr, f, mask\u001b[39m.\u001b[39;49mview(np\u001b[39m.\u001b[39;49muint8), convert)\n\u001b[1;32m     62\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mAttributeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m     \u001b[39m# Reraise the exception if callable `f` got wrong number of args.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \u001b[39m# The user may want to be warned by this, instead of getting NaN\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     p_err \u001b[39m=\u001b[39m (\n\u001b[1;32m     66\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m((takes)|(missing)) (?(2)from \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ to )?\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(?(3)required )positional arguments?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m     )\n",
      "File \u001b[0;32mpandas/_libs/lib.pyx:2421\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer_mask\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/strings/object_array.py:264\u001b[0m, in \u001b[0;36mObjectStringArrayMixin._str_get.<locals>.f\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mf\u001b[39m(x):\n\u001b[0;32m--> 264\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39;49m(x, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    265\u001b[0m         \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39mget(i)\n\u001b[1;32m    266\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(x) \u001b[39m>\u001b[39m i \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(x):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"loading csv.....\")\n",
    "train_df = pd.read_csv(Config.TRAIN_FILE)\n",
    "test_df = pd.read_csv(Config.TEST_FILE)\n",
    "\n",
    "test_df = test_df[test_df['answerCode']!=-1]\n",
    "train_df = pd.concat([train_df, test_df])\n",
    "\n",
    "train_df = train_df.sort_values(by=['userID', 'Timestamp']).reset_index(drop=True)\n",
    "\n",
    "print(\"shape of (train) dataframe :\", train_df.shape)\n",
    "\n",
    "train_df['grade'] = (train_df['assessmentItemID'].str[2]).astype('int')\n",
    "train_df['testId'] = (train_df['assessmentItemID'].str[2] + train_df['assessmentItemID'].str[4:7])\n",
    "train_df[\"Timestamp\"] = pd.to_datetime(train_df[\"Timestamp\"])\n",
    "\n",
    "ItemID2idx = {v:k for k,v in enumerate(train_df['assessmentItemID'].unique())} # 9453개\n",
    "testId2idx = {v:k for k,v in enumerate(train_df['testId'].unique())} # 1536개\n",
    "Tag2idx = {v:k for k,v in enumerate(train_df['KnowledgeTag'].unique())} # 911개\n",
    "\n",
    "train_df['assessmentItemID'] = train_df['assessmentItemID'].map(ItemID2idx)\n",
    "train_df['testId'] = train_df['testId'].map(testId2idx)\n",
    "train_df['KnowledgeTag'] = train_df['KnowledgeTag'].map(Tag2idx)\n",
    "\n",
    "#필요 Column 생성 - 'prior_question_elapsed_time'\n",
    "train_df['prior_question_elapsed_time'] = 0\n",
    "\n",
    "for i in range(len(train_df)-1):\n",
    "    if train_df.at[i,'userID'] == train_df.at[(i+1),'userID']:\n",
    "        train_df.at[i,'prior_question_elapsed_time'] = (train_df.at[i+1,'Timestamp']-train_df.at[i,'Timestamp']).total_seconds()\n",
    "    if i %100000 == 0:\n",
    "        print(f'train elapsetime process  {i}/{len(train_df)} done ')\n",
    "\n",
    "train_df = train_df.drop(['Timestamp'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======origin length      : 2525956======\n",
      "START 1th AUGMENTATION\n",
      "END   1th AUGMENTATION\n",
      "START 2th AUGMENTATION\n",
      "END   2th AUGMENTATION\n",
      "START 3th AUGMENTATION\n",
      "END   3th AUGMENTATION\n",
      "START 4th AUGMENTATION\n",
      "END   4th AUGMENTATION\n",
      "START 5th AUGMENTATION\n",
      "END   5th AUGMENTATION\n",
      "START 6th AUGMENTATION\n",
      "END   6th AUGMENTATION\n",
      "START 7th AUGMENTATION\n",
      "END   7th AUGMENTATION\n",
      "START 8th AUGMENTATION\n",
      "END   8th AUGMENTATION\n",
      "START 9th AUGMENTATION\n",
      "END   9th AUGMENTATION\n",
      "START 10th AUGMENTATION\n",
      "END   10th AUGMENTATION\n",
      "======after augmentation : 27376207======\n"
     ]
    }
   ],
   "source": [
    "#data augmentation 증강 수 aug에 입력하자\n",
    "aug = 10\n",
    "\n",
    "\n",
    "print(f'======origin length      : {len(train_df)}======')\n",
    "train_origin = train_df.copy()\n",
    "train_aug = train_df.copy()\n",
    "n= 1\n",
    "for i in range(aug):\n",
    "    print(f'START {n}th AUGMENTATION')\n",
    "    tem = train_origin.drop_duplicates(subset = [\"userID\"],keep = \"last\")\n",
    "    train_origin = train_origin.drop(index=tem.index)\n",
    "    train_origin['userID'] += 7442 #train_origin['userID'].nunique()+1\n",
    "    train_aug = pd.concat([train_aug, train_origin], axis = 0)\n",
    "    print(f'END   {n}th AUGMENTATION')\n",
    "    n += 1\n",
    "print(f'======after augmentation : {len(train_aug)}======')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug.to_csv(f'/opt/ml/input/data/train_saint_dkt_x{aug}.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test csv 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading csv.....\n",
      "shape of (train) dataframe : (74876098, 7)\n",
      "test elpasetime process  0/260114 done \n",
      "test elpasetime process  100000/260114 done \n",
      "test elpasetime process  200000/260114 done \n"
     ]
    }
   ],
   "source": [
    "print(\"loading csv.....\")\n",
    "test_df = pd.read_csv(Config.TEST_FILE)\n",
    "print(\"shape of (train) dataframe :\", train_df.shape)\n",
    "\n",
    "\n",
    "test_df['grade'] = (test_df['assessmentItemID'].str[2]).astype('int')\n",
    "test_df['testId'] = test_df['assessmentItemID'].str[2] + test_df['assessmentItemID'].str[4:7]\n",
    "test_df[\"Timestamp\"] = pd.to_datetime(test_df[\"Timestamp\"])\n",
    "\n",
    "\n",
    "ItemID2idx = {v:k for k,v in enumerate(test_df['assessmentItemID'].unique())} # 9453개\n",
    "testId2idx = {v:k for k,v in enumerate(test_df['testId'].unique())} # 1536개\n",
    "Tag2idx = {v:k for k,v in enumerate(test_df['KnowledgeTag'].unique())} # 911개\n",
    "\n",
    "test_df['assessmentItemID'] = test_df['assessmentItemID'].map(ItemID2idx)\n",
    "test_df['testId'] = test_df['testId'].map(testId2idx)\n",
    "test_df['KnowledgeTag'] = test_df['KnowledgeTag'].map(Tag2idx)\n",
    "\n",
    "\n",
    "\n",
    "#필요 Column 생성 - 'prior_question_elapsed_time'\n",
    "test_df['prior_question_elapsed_time'] = 0\n",
    "\n",
    "\n",
    "for i in range(len(test_df)-1):\n",
    "    if test_df.at[i,'userID'] == test_df.at[(i+1),'userID']:\n",
    "        test_df.at[i,'prior_question_elapsed_time'] = (test_df.at[i+1,'Timestamp']-test_df.at[i,'Timestamp']).total_seconds()\n",
    "    if i %100000 == 0:\n",
    "        print(f'test elpasetime process  {i}/{len(test_df)} done ')\n",
    "        \n",
    "test_df = test_df.drop(['Timestamp'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(f'/opt/ml/input/data/test_saint_dkt.csv',index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train test 병합 안하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading csv.....\n",
      "shape of (train) dataframe : (2266586, 6)\n",
      "train elapsetime process  0/2266586 done \n",
      "train elapsetime process  100000/2266586 done \n",
      "train elapsetime process  200000/2266586 done \n",
      "train elapsetime process  300000/2266586 done \n",
      "train elapsetime process  400000/2266586 done \n",
      "train elapsetime process  500000/2266586 done \n",
      "train elapsetime process  600000/2266586 done \n",
      "train elapsetime process  700000/2266586 done \n",
      "train elapsetime process  800000/2266586 done \n",
      "train elapsetime process  900000/2266586 done \n",
      "train elapsetime process  1000000/2266586 done \n",
      "train elapsetime process  1100000/2266586 done \n",
      "train elapsetime process  1200000/2266586 done \n",
      "train elapsetime process  1300000/2266586 done \n",
      "train elapsetime process  1400000/2266586 done \n",
      "train elapsetime process  1500000/2266586 done \n",
      "train elapsetime process  1600000/2266586 done \n",
      "train elapsetime process  1700000/2266586 done \n",
      "train elapsetime process  1800000/2266586 done \n",
      "train elapsetime process  1900000/2266586 done \n",
      "train elapsetime process  2000000/2266586 done \n",
      "train elapsetime process  2100000/2266586 done \n",
      "train elapsetime process  2200000/2266586 done \n",
      "test elpasetime process  0/260114 done \n",
      "test elpasetime process  100000/260114 done \n",
      "test elpasetime process  200000/260114 done \n"
     ]
    }
   ],
   "source": [
    "print(\"loading csv.....\")\n",
    "train_df = pd.read_csv(Config.TRAIN_FILE)\n",
    "test_df = pd.read_csv(Config.TEST_FILE)\n",
    "print(\"shape of (train) dataframe :\", train_df.shape)\n",
    "\n",
    "# 기본적인 데이터 전처리 부분.\n",
    "# column : userID assessmentItemID  testId answerCode Timestamp KnowledgeTag grade\n",
    "\n",
    "train_df['grade'] = (train_df['assessmentItemID'].str[2]).astype('int')\n",
    "train_df['testId'] = (train_df['assessmentItemID'].str[2] + train_df['assessmentItemID'].str[4:7])\n",
    "train_df[\"Timestamp\"] = pd.to_datetime(train_df[\"Timestamp\"])\n",
    "\n",
    "test_df['grade'] = (test_df['assessmentItemID'].str[2]).astype('int')\n",
    "test_df['testId'] = test_df['assessmentItemID'].str[2] + test_df['assessmentItemID'].str[4:7]\n",
    "test_df[\"Timestamp\"] = pd.to_datetime(test_df[\"Timestamp\"])\n",
    "\n",
    "ItemID2idx = {v:k for k,v in enumerate(train_df['assessmentItemID'].unique())} # 9453개\n",
    "testId2idx = {v:k for k,v in enumerate(train_df['testId'].unique())} # 1536개\n",
    "Tag2idx = {v:k for k,v in enumerate(train_df['KnowledgeTag'].unique())} # 911개\n",
    "\n",
    "# ItemID2idx = {v:k for k,v in enumerate(test_df['assessmentItemID'].unique())} # 9453개\n",
    "# testId2idx = {v:k for k,v in enumerate(test_df['testId'].unique())} # 1536개\n",
    "# Tag2idx = {v:k for k,v in enumerate(test_df['KnowledgeTag'].unique())} # 911개\n",
    "\n",
    "\n",
    "train_df['assessmentItemID'] = train_df['assessmentItemID'].map(ItemID2idx)\n",
    "train_df['testId'] = train_df['testId'].map(testId2idx)\n",
    "train_df['KnowledgeTag'] = train_df['KnowledgeTag'].map(Tag2idx)\n",
    "\n",
    "test_df['assessmentItemID'] = test_df['assessmentItemID'].map(ItemID2idx)\n",
    "test_df['testId'] = test_df['testId'].map(testId2idx)\n",
    "test_df['KnowledgeTag'] = test_df['KnowledgeTag'].map(Tag2idx)\n",
    "\n",
    "\n",
    "\n",
    "# #필요 Column 생성 - 'content_type_id'\n",
    "# train_df['content_type_id'] = 0\n",
    "# test_df['content_type_id'] = 0\n",
    "\n",
    "#필요 Column 생성 - 'prior_question_elapsed_time'\n",
    "train_df['prior_question_elapsed_time'] = 0\n",
    "test_df['prior_question_elapsed_time'] = 0\n",
    "\n",
    "for i in range(len(train_df)-1):\n",
    "    if train_df.at[i,'userID'] == train_df.at[(i+1),'userID']:\n",
    "        train_df.at[i,'prior_question_elapsed_time'] = (train_df.at[i+1,'Timestamp']-train_df.at[i,'Timestamp']).total_seconds()\n",
    "    if i %100000 == 0:\n",
    "        print(f'train elapsetime process  {i}/{len(train_df)} done ')\n",
    "\n",
    "for i in range(len(test_df)-1):\n",
    "    if test_df.at[i,'userID'] == test_df.at[(i+1),'userID']:\n",
    "        test_df.at[i,'prior_question_elapsed_time'] = (test_df.at[i+1,'Timestamp']-test_df.at[i,'Timestamp']).total_seconds()\n",
    "    if i %100000 == 0:\n",
    "        print(f'test elpasetime process  {i}/{len(test_df)} done ')\n",
    "        \n",
    "train_df = train_df.drop(['Timestamp'], axis=1)\n",
    "test_df = test_df.drop(['Timestamp'], axis=1)\n",
    "\n",
    "# # 필요 Column 생성 - 'task_container_id'\n",
    "# train_df['task_container_id'] = 0\n",
    "# test_df['task_container_id'] = 0\n",
    "\n",
    "# for i in range(1,len(train_df)):\n",
    "#     if train_df.at[i,'userID'] == train_df.at[i-1,'userID']:\n",
    "#         train_df.at[i,'task_container_id'] = train_df.at[i-1,'task_container_id'] + 1\n",
    "#     if i %100000 == 0:\n",
    "#         print(f'train task process  {i}/{len(train_df)} done ')\n",
    "\n",
    "# for i in range(1,len(test_df)):\n",
    "#     if test_df.at[i,'userID'] == test_df.at[i-1,'userID']:\n",
    "#         test_df.at[i,'task_container_id'] = test_df.at[i-1,'task_container_id'] + 1\n",
    "#     if i %100000 == 0:\n",
    "#         print(f'test task process  {i}/{len(test_df)} done ')\n",
    "\n",
    "# # SAINT에 필요한 것만 남김\n",
    "# train_df = train_df.drop(['testId','KnowledgeTag','grade'], axis=1)\n",
    "# test_df = test_df.drop(['testId','KnowledgeTag','grade'], axis=1)\n",
    "\n",
    "# # change to SAINT Input\n",
    "# train_df  = train_df.rename(columns = {'userID': 'user_id', 'Timestamp':'timestamp', 'assessmentItemID' : 'content_id',\n",
    "#                            'answerCode' : 'answered_correctly'  })\n",
    "# test_df  = test_df.rename(columns = {'userID': 'user_id', 'Timestamp':'timestamp', 'assessmentItemID' : 'content_id',\n",
    "#                            'answerCode' : 'answered_correctly'  })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======origin length      : 2266586======\n",
      "START 1th AUGMENTATION\n",
      "END   1th AUGMENTATION\n",
      "START 2th AUGMENTATION\n",
      "END   2th AUGMENTATION\n",
      "START 3th AUGMENTATION\n",
      "END   3th AUGMENTATION\n",
      "START 4th AUGMENTATION\n",
      "END   4th AUGMENTATION\n",
      "START 5th AUGMENTATION\n",
      "END   5th AUGMENTATION\n",
      "START 6th AUGMENTATION\n",
      "END   6th AUGMENTATION\n",
      "START 7th AUGMENTATION\n",
      "END   7th AUGMENTATION\n",
      "START 8th AUGMENTATION\n",
      "END   8th AUGMENTATION\n",
      "START 9th AUGMENTATION\n",
      "END   9th AUGMENTATION\n",
      "START 10th AUGMENTATION\n",
      "END   10th AUGMENTATION\n",
      "START 11th AUGMENTATION\n",
      "END   11th AUGMENTATION\n",
      "START 12th AUGMENTATION\n",
      "END   12th AUGMENTATION\n",
      "START 13th AUGMENTATION\n",
      "END   13th AUGMENTATION\n",
      "START 14th AUGMENTATION\n",
      "END   14th AUGMENTATION\n",
      "START 15th AUGMENTATION\n",
      "END   15th AUGMENTATION\n",
      "START 16th AUGMENTATION\n",
      "END   16th AUGMENTATION\n",
      "START 17th AUGMENTATION\n",
      "END   17th AUGMENTATION\n",
      "START 18th AUGMENTATION\n",
      "END   18th AUGMENTATION\n",
      "START 19th AUGMENTATION\n",
      "END   19th AUGMENTATION\n",
      "START 20th AUGMENTATION\n",
      "END   20th AUGMENTATION\n",
      "START 21th AUGMENTATION\n",
      "END   21th AUGMENTATION\n",
      "START 22th AUGMENTATION\n",
      "END   22th AUGMENTATION\n",
      "START 23th AUGMENTATION\n",
      "END   23th AUGMENTATION\n",
      "START 24th AUGMENTATION\n",
      "END   24th AUGMENTATION\n",
      "START 25th AUGMENTATION\n",
      "END   25th AUGMENTATION\n",
      "START 26th AUGMENTATION\n",
      "END   26th AUGMENTATION\n",
      "START 27th AUGMENTATION\n",
      "END   27th AUGMENTATION\n",
      "START 28th AUGMENTATION\n",
      "END   28th AUGMENTATION\n",
      "START 29th AUGMENTATION\n",
      "END   29th AUGMENTATION\n",
      "START 30th AUGMENTATION\n",
      "END   30th AUGMENTATION\n",
      "======after augmentation : 67177754======\n"
     ]
    }
   ],
   "source": [
    "#data augmentation 증강 수 aug에 입력하자\n",
    "aug = 30\n",
    "\n",
    "\n",
    "print(f'======origin length      : {len(train_df)}======')\n",
    "train_origin = train_df.copy()\n",
    "train_aug = train_df.copy()\n",
    "n= 1\n",
    "for i in range(aug):\n",
    "    print(f'START {n}th AUGMENTATION')\n",
    "    tem = train_origin.drop_duplicates(subset = [\"userID\"],keep = \"last\")\n",
    "    train_origin = train_origin.drop(index=tem.index)\n",
    "    train_origin['userID'] += 7442 #train_origin['userID'].nunique()+1\n",
    "    train_aug = pd.concat([train_aug, train_origin], axis = 0)\n",
    "    print(f'END   {n}th AUGMENTATION')\n",
    "    n += 1\n",
    "print(f'======after augmentation : {len(train_aug)}======')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug.to_csv(f'/opt/ml/input/data/train_saint_dkt_notest_x{aug}.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
