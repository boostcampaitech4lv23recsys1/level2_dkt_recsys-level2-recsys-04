{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    device = torch.device(\"cuda\")\n",
    "    MAX_SEQ = 100\n",
    "    EMBED_DIMS = 512\n",
    "    ENC_HEADS = DEC_HEADS = 8\n",
    "    NUM_ENCODER = NUM_DECODER = 4\n",
    "    BATCH_SIZE = 32\n",
    "    TRAIN_FILE = \"/opt/ml/input/data/train_data.csv\"\n",
    "    TEST_FILE = \"/opt/ml/input/data/test_data.csv\"\n",
    "    TOTAL_EXE = 13523\n",
    "    TOTAL_CAT = 10000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.데이터 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존 SAINT : train의 Column 종류\n",
    "\n",
    "row_id, timestamp, user_id, content_id, content_type_id,\n",
    "\n",
    "task_container_id, user_answer, answered_correctly, prior_question_elapsed_time, prior_question_had_explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리 DKT\n",
    "\n",
    "userID,assessmentItemID,testId,answerCode,Timestamp,KnowledgeTag\n",
    "\n",
    "공통적으로 User id, timestamp 있고\n",
    "\n",
    "콘텐츠적으로 나누면 우리 dkt 는 3개정도 분류될거 같고,\n",
    "\n",
    "saint+이기 때문에 time 에 관해서 더 프로세싱하면 될거 같다.(elapsed, Lagged time 은 데이터상으로 구별하기 힘들다.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "'timestamp': 'int64',  -> Timestamp\n",
    "\n",
    "'user_id': 'int32',  -> userID\n",
    "\n",
    "'content_id': 'int16' -> assessmentItemID\n",
    "\n",
    "'answered_correctly': 'int8',   -> answerCode\n",
    "\n",
    "\n",
    "\n",
    "\"content_type_id\": \"int8\", -> 0으로 생성\n",
    "\n",
    "\n",
    "\n",
    "\"prior_question_elapsed_time\": \"float32\", -> Timestamp 변형\n",
    "\n",
    "\"task_container_id\": \"int16\" -> 누적 개수 (새로 생성 필요)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading csv.....\n",
      "shape of (train) dataframe : (2266586, 6)\n",
      "train elapsetime process  0/2266586 done \n",
      "train elapsetime process  100000/2266586 done \n"
     ]
    }
   ],
   "source": [
    "print(\"loading csv.....\")\n",
    "train_df = pd.read_csv(Config.TRAIN_FILE)\n",
    "test_df = pd.read_csv(Config.TEST_FILE)\n",
    "print(\"shape of (train) dataframe :\", train_df.shape)\n",
    "\n",
    "# 기본적인 데이터 전처리 부분.\n",
    "# column : userID assessmentItemID  testId answerCode Timestamp KnowledgeTag grade\n",
    "\n",
    "train_df['grade'] = (train_df['assessmentItemID'].str[2]).astype('int')\n",
    "train_df['testId'] = (train_df['assessmentItemID'].str[2] + train_df['assessmentItemID'].str[4:7])\n",
    "train_df[\"Timestamp\"] = pd.to_datetime(train_df[\"Timestamp\"])\n",
    "\n",
    "test_df['grade'] = (test_df['assessmentItemID'].str[2]).astype('int')\n",
    "test_df['testId'] = test_df['assessmentItemID'].str[2] + test_df['assessmentItemID'].str[4:7]\n",
    "test_df[\"Timestamp\"] = pd.to_datetime(test_df[\"Timestamp\"])\n",
    "\n",
    "ItemID2idx = {v:k for k,v in enumerate(train_df['assessmentItemID'].unique())} # 9453개\n",
    "testId2idx = {v:k for k,v in enumerate(train_df['testId'].unique())} # 1536개\n",
    "Tag2idx = {v:k for k,v in enumerate(train_df['KnowledgeTag'].unique())} # 911개\n",
    "\n",
    "# ItemID2idx = {v:k for k,v in enumerate(test_df['assessmentItemID'].unique())} # 9453개\n",
    "# testId2idx = {v:k for k,v in enumerate(test_df['testId'].unique())} # 1536개\n",
    "# Tag2idx = {v:k for k,v in enumerate(test_df['KnowledgeTag'].unique())} # 911개\n",
    "\n",
    "\n",
    "train_df['assessmentItemID'] = train_df['assessmentItemID'].map(ItemID2idx)\n",
    "train_df['testId'] = train_df['testId'].map(testId2idx)\n",
    "train_df['KnowledgeTag'] = train_df['KnowledgeTag'].map(Tag2idx)\n",
    "\n",
    "test_df['assessmentItemID'] = test_df['assessmentItemID'].map(ItemID2idx)\n",
    "test_df['testId'] = test_df['testId'].map(testId2idx)\n",
    "test_df['KnowledgeTag'] = test_df['KnowledgeTag'].map(Tag2idx)\n",
    "\n",
    "\n",
    "\n",
    "# #필요 Column 생성 - 'content_type_id'\n",
    "# train_df['content_type_id'] = 0\n",
    "# test_df['content_type_id'] = 0\n",
    "\n",
    "#필요 Column 생성 - 'prior_question_elapsed_time'\n",
    "train_df['prior_question_elapsed_time'] = 0\n",
    "test_df['prior_question_elapsed_time'] = 0\n",
    "\n",
    "for i in range(len(train_df)-1):\n",
    "    if train_df.at[i,'userID'] == train_df.at[(i+1),'userID']:\n",
    "        train_df.at[i,'prior_question_elapsed_time'] = (train_df.at[i+1,'Timestamp']-train_df.at[i,'Timestamp']).total_seconds()\n",
    "    if i %100000 == 0:\n",
    "        print(f'train elapsetime process  {i}/{len(train_df)} done ')\n",
    "\n",
    "for i in range(len(test_df)-1):\n",
    "    if test_df.at[i,'userID'] == test_df.at[(i+1),'userID']:\n",
    "        test_df.at[i,'prior_question_elapsed_time'] = (test_df.at[i+1,'Timestamp']-test_df.at[i,'Timestamp']).total_seconds()\n",
    "    if i %100000 == 0:\n",
    "        print(f'test elpasetime process  {i}/{len(test_df)} done ')\n",
    "        \n",
    "train_df = train_df.drop(['Timestamp'], axis=1)\n",
    "test_df = test_df.drop(['Timestamp'], axis=1)\n",
    "\n",
    "# # 필요 Column 생성 - 'task_container_id'\n",
    "# train_df['task_container_id'] = 0\n",
    "# test_df['task_container_id'] = 0\n",
    "\n",
    "# for i in range(1,len(train_df)):\n",
    "#     if train_df.at[i,'userID'] == train_df.at[i-1,'userID']:\n",
    "#         train_df.at[i,'task_container_id'] = train_df.at[i-1,'task_container_id'] + 1\n",
    "#     if i %100000 == 0:\n",
    "#         print(f'train task process  {i}/{len(train_df)} done ')\n",
    "\n",
    "# for i in range(1,len(test_df)):\n",
    "#     if test_df.at[i,'userID'] == test_df.at[i-1,'userID']:\n",
    "#         test_df.at[i,'task_container_id'] = test_df.at[i-1,'task_container_id'] + 1\n",
    "#     if i %100000 == 0:\n",
    "#         print(f'test task process  {i}/{len(test_df)} done ')\n",
    "\n",
    "# # SAINT에 필요한 것만 남김\n",
    "# train_df = train_df.drop(['testId','KnowledgeTag','grade'], axis=1)\n",
    "# test_df = test_df.drop(['testId','KnowledgeTag','grade'], axis=1)\n",
    "\n",
    "# # change to SAINT Input\n",
    "# train_df  = train_df.rename(columns = {'userID': 'user_id', 'Timestamp':'timestamp', 'assessmentItemID' : 'content_id',\n",
    "#                            'answerCode' : 'answered_correctly'  })\n",
    "# test_df  = test_df.rename(columns = {'userID': 'user_id', 'Timestamp':'timestamp', 'assessmentItemID' : 'content_id',\n",
    "#                            'answerCode' : 'answered_correctly'  })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======origin length      : 2266586======\n",
      "START 5th AUGMENTATION\n",
      "END   5th AUGMENTATION\n",
      "START 6th AUGMENTATION\n",
      "END   6th AUGMENTATION\n",
      "START 7th AUGMENTATION\n",
      "END   7th AUGMENTATION\n",
      "START 8th AUGMENTATION\n",
      "END   8th AUGMENTATION\n",
      "START 9th AUGMENTATION\n",
      "END   9th AUGMENTATION\n",
      "======after augmentation : 13499046======\n"
     ]
    }
   ],
   "source": [
    "#data augmentation 증강 수 aug에 입력하자\n",
    "aug = 5  \n",
    "\n",
    "\n",
    "print(f'======origin length      : {len(train_df)}======')\n",
    "train_origin = train_df.copy()\n",
    "n= 1\n",
    "for i in range(aug):\n",
    "    print(f'START {n}th AUGMENTATION')\n",
    "    tem = train_origin.drop_duplicates(subset = [\"userID\"],keep = \"last\")\n",
    "    train_origin = train_origin.drop(index=tem.index)\n",
    "    train_origin['userID'] += 7442 #train_origin['userID'].nunique()+1\n",
    "    train_df = pd.concat([train_df, train_origin], axis = 0)\n",
    "    print(f'END   {n}th AUGMENTATION')\n",
    "    n += 1\n",
    "print(f'======after augmentation : {len(train_df)}======')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(f'/opt/ml/input/data/train_saint_dkt_x{aug}.csv',index = False)\n",
    "test_df.to_csv(f'/opt/ml/input/data/test_saint_dkt.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
