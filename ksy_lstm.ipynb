{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dkt 가상 환경에서 진행.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "try:\n",
    "    from transformers.modeling_bert import BertConfig, BertEncoder, BertModel\n",
    "except:\n",
    "    from transformers.models.bert.modeling_bert import (\n",
    "        BertConfig,\n",
    "        BertEncoder,\n",
    "        BertModel,\n",
    "    )\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "\n",
    "path = '/opt/ml/input/data/'\n",
    "\n",
    "train_path = os.path.join(path, 'train_data.csv')\n",
    "test_path = os.path.join(path, 'test_data.csv')\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "test = pd.read_csv(test_path)\n",
    "\n",
    "class args: \n",
    "    gpu_idx = 0\n",
    "    device = torch.device(\"cuda:{}\".format(gpu_idx) if torch.cuda.is_available() else \"cpu\")\n",
    "    seed = 64\n",
    "    max_seq_len = 20 # 최대 문장 길이.\n",
    "    num_workers = 1\n",
    "    batch_size = 128\n",
    "    n_epochs = 10\n",
    "    hidden_dim = 8\n",
    "    n_layers = 3\n",
    "    lr = 0.01\n",
    "    clip_grad = 10\n",
    "    log_steps = 200\n",
    "    augmentation = 50 # 데이터 증강 횟수.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>assessmentItemID</th>\n",
       "      <th>testId</th>\n",
       "      <th>answerCode</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>KnowledgeTag</th>\n",
       "      <th>grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-24 00:17:11</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-24 00:17:14</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-24 00:17:22</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-24 00:17:29</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-24 00:17:36</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  assessmentItemID  testId  answerCode           Timestamp  \\\n",
       "0       0                 0       0           1 2020-03-24 00:17:11   \n",
       "1       0                 1       0           1 2020-03-24 00:17:14   \n",
       "2       0                 2       0           1 2020-03-24 00:17:22   \n",
       "3       0                 3       0           1 2020-03-24 00:17:29   \n",
       "4       0                 4       0           1 2020-03-24 00:17:36   \n",
       "\n",
       "   KnowledgeTag  grade  \n",
       "0             0      6  \n",
       "1             1      6  \n",
       "2             1      6  \n",
       "3             1      6  \n",
       "4             1      6  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기본적인 데이터 전처리 부분.\n",
    "train['grade'] = (train['assessmentItemID'].str[2]).astype('int')\n",
    "test['grade'] = (test['assessmentItemID'].str[2]).astype('int')\n",
    "train['testId'] = (train['assessmentItemID'].str[2] + train['assessmentItemID'].str[4:7])\n",
    "test['testId'] = test['assessmentItemID'].str[2] + test['assessmentItemID'].str[4:7]\n",
    "train[\"Timestamp\"] = pd.to_datetime(train[\"Timestamp\"])\n",
    "test[\"Timestamp\"] = pd.to_datetime(test[\"Timestamp\"])\n",
    "\n",
    "ItemID2idx = {v:k for k,v in enumerate(train['assessmentItemID'].unique())} # 9453개\n",
    "testId2idx = {v:k for k,v in enumerate(train['testId'].unique())} # 1536개\n",
    "Tag2idx = {v:k for k,v in enumerate(train['KnowledgeTag'].unique())} # 911개\n",
    "\n",
    "args.n_item = train['assessmentItemID'].nunique()\n",
    "args.n_test = train['testId'].nunique()\n",
    "args.n_tag = train['KnowledgeTag'].nunique()\n",
    "args.n_grade = train['grade'].nunique()\n",
    "\n",
    "\n",
    "train['assessmentItemID'] = train['assessmentItemID'].map(ItemID2idx)\n",
    "train['testId'] = train['testId'].map(testId2idx)\n",
    "train['KnowledgeTag'] = train['KnowledgeTag'].map(Tag2idx)\n",
    "\n",
    "test['assessmentItemID'] = test['assessmentItemID'].map(ItemID2idx)\n",
    "test['testId'] = test['testId'].map(testId2idx)\n",
    "test['KnowledgeTag'] = test['KnowledgeTag'].map(Tag2idx)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>assessmentItemID</th>\n",
       "      <th>testId</th>\n",
       "      <th>answerCode</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>KnowledgeTag</th>\n",
       "      <th>grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-24 00:17:11</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-24 00:17:14</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-24 00:17:22</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-24 00:17:29</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-24 00:17:36</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2259582</th>\n",
       "      <td>318988</td>\n",
       "      <td>8051</td>\n",
       "      <td>1301</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-06-06 02:53:11</td>\n",
       "      <td>689</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2259583</th>\n",
       "      <td>318988</td>\n",
       "      <td>8055</td>\n",
       "      <td>1301</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-06-06 02:53:14</td>\n",
       "      <td>690</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2259584</th>\n",
       "      <td>318988</td>\n",
       "      <td>8057</td>\n",
       "      <td>1301</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-06-06 02:53:18</td>\n",
       "      <td>682</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2259585</th>\n",
       "      <td>318988</td>\n",
       "      <td>8058</td>\n",
       "      <td>1301</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-06-06 02:53:22</td>\n",
       "      <td>691</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2261558</th>\n",
       "      <td>319075</td>\n",
       "      <td>5069</td>\n",
       "      <td>814</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-10-13 03:21:33</td>\n",
       "      <td>262</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107340628 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         userID  assessmentItemID  testId  answerCode           Timestamp  \\\n",
       "0             0                 0       0           1 2020-03-24 00:17:11   \n",
       "1             0                 1       0           1 2020-03-24 00:17:14   \n",
       "2             0                 2       0           1 2020-03-24 00:17:22   \n",
       "3             0                 3       0           1 2020-03-24 00:17:29   \n",
       "4             0                 4       0           1 2020-03-24 00:17:36   \n",
       "...         ...               ...     ...         ...                 ...   \n",
       "2259582  318988              8051    1301           1 2020-06-06 02:53:11   \n",
       "2259583  318988              8055    1301           1 2020-06-06 02:53:14   \n",
       "2259584  318988              8057    1301           1 2020-06-06 02:53:18   \n",
       "2259585  318988              8058    1301           1 2020-06-06 02:53:22   \n",
       "2261558  319075              5069     814           1 2020-10-13 03:21:33   \n",
       "\n",
       "         KnowledgeTag  grade  \n",
       "0                   0      6  \n",
       "1                   1      6  \n",
       "2                   1      6  \n",
       "3                   1      6  \n",
       "4                   1      6  \n",
       "...               ...    ...  \n",
       "2259582           689      7  \n",
       "2259583           690      7  \n",
       "2259584           682      7  \n",
       "2259585           691      7  \n",
       "2261558           262      4  \n",
       "\n",
       "[107340628 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 증강 부분. 선태 코드 참고했으.\n",
    "# 근데 이런식으로 증강하게 되면 이전에 1문제 풀었는데 이번 문제 맞추라는 식의 데이터가 생김\n",
    "# test 데이터는 내가 알기로 1개 빼고는 이전 푼 문제가 15문제니 만큼 이에 잘 맞추는 식으로 변형해도 좋을듯.\n",
    "# 즉 증강은 이대로 하되 후처리를 통해 이전에 1~5문제 등 적게 푼 사람 걸러내도 좋을듯.(귀찮아서 안한거 아님. 아마도.)\n",
    "train_origin = train.copy()\n",
    "train_new = train.copy()\n",
    "for i in range(args.augmentation):\n",
    "    tem = train_origin.drop_duplicates(subset = [\"userID\"],keep = \"last\")\n",
    "    train_origin = train_origin.drop(index=tem.index)\n",
    "    train_origin['userID'] += train_origin['userID'].nunique()\n",
    "    train_new = pd.concat([train_new, train_origin], axis = 0)\n",
    "train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베이스라인과 유사. 쓰는 데이터 조금 달라짐.\n",
    "columns = [\"userID\", \"assessmentItemID\", \"testId\", \"answerCode\", \"KnowledgeTag\", 'grade']\n",
    "train_data = (\n",
    "            train_new[columns] # train_new : 데이터 증강.\n",
    "            .groupby(\"userID\")\n",
    "            .apply(\n",
    "                lambda r: (\n",
    "                    r[\"testId\"].values,\n",
    "                    r[\"assessmentItemID\"].values,\n",
    "                    r[\"KnowledgeTag\"].values,\n",
    "                    r[\"grade\"].values,\n",
    "                    r[\"answerCode\"].values,\n",
    "                )\n",
    "            )\n",
    "        ).values\n",
    "\n",
    "test_data = (\n",
    "            test[columns]\n",
    "            .groupby(\"userID\")\n",
    "            .apply(\n",
    "                lambda r: (\n",
    "                    r[\"testId\"].values,\n",
    "                    r[\"assessmentItemID\"].values,\n",
    "                    r[\"KnowledgeTag\"].values,\n",
    "                    r[\"grade\"].values,\n",
    "                    r[\"answerCode\"].values,\n",
    "                )\n",
    "            )\n",
    "        ).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.9 # 데이터 증강 했으니 10%만 써도 test 데이터 개수가 충분한듯.\n",
    "\n",
    "random.seed(args.seed)\n",
    "random.shuffle(train_data)\n",
    "\n",
    "size = int(len(train_data) * ratio)\n",
    "valid_data = train_data[size:]\n",
    "train_data = train_data[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DKTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, args):\n",
    "        self.data = data\n",
    "        self.args = args\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data[index]\n",
    "\n",
    "        # 각 data의 sequence length\n",
    "        seq_len = len(row[0])\n",
    "\n",
    "        test, item, tag, grade, correct = row[0], row[1], row[2], row[3], row[4]\n",
    "\n",
    "        cate_cols = [test, item, tag, grade, correct]\n",
    "\n",
    "        # max seq len을 고려하여서 이보다 길면 자르고 아닐 경우 그대로 냅둔다\n",
    "        # max seq len 길이를 잘 조절하는 것도 관건 (실험을 통해 해결해야할 문제)\n",
    "        # defalut가 20인데 이전 문제 정보를 더 사용하고 싶진 않은가? + 너무 키우면 이전 문제 정보를 너무 많이 사용하나?\n",
    "        if seq_len > self.args.max_seq_len:\n",
    "            for i, col in enumerate(cate_cols):\n",
    "                cate_cols[i] = col[-self.args.max_seq_len :]\n",
    "            mask = np.ones(self.args.max_seq_len, dtype=np.int16)\n",
    "        else:\n",
    "            mask = np.zeros(self.args.max_seq_len, dtype=np.int16)\n",
    "            mask[-seq_len:] = 1\n",
    "\n",
    "        # mask도 columns 목록에 포함시킴, mask는 이전 문제 정보가 max seq len(20) 보다 작으면 이를 알려주는 변수.\n",
    "        cate_cols.append(mask)\n",
    "\n",
    "        # np.array -> torch.tensor 형변환\n",
    "        for i, col in enumerate(cate_cols):\n",
    "            cate_cols[i] = torch.tensor(col)\n",
    "\n",
    "        return cate_cols\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "train_dataset = DKTDataset(train_data, args)\n",
    "valid_dataset = DKTDataset(valid_data, args)\n",
    "test_dataset = DKTDataset(test_data, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    col_n = len(batch[0]) # column 요소의 수 (defalut : 6, mask까지)\n",
    "    col_list = [[] for _ in range(col_n)]\n",
    "    # 입력된 길이의 수. (미리 정해놓은 만큼 DKTDataset에서 이미 조정함)\n",
    "    max_seq_len = len(batch[0][-1]) \n",
    "\n",
    "    # batch의 값들을 각 column끼리 그룹화\n",
    "    for row in batch:\n",
    "        for i, col in enumerate(row):\n",
    "            pre_padded = torch.zeros(max_seq_len)\n",
    "            pre_padded[-len(col) :] = col\n",
    "            col_list[i].append(pre_padded)\n",
    "\n",
    "    for i, _ in enumerate(col_list):\n",
    "        # torch.stack : [tensor(20), tensor(20), ..] => tensor(batch_size, 20)\n",
    "        col_list[i] = torch.stack(col_list[i])\n",
    "\n",
    "    return tuple(col_list) # column수 * tensor(batch_size, max_seq_len)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    num_workers=args.num_workers,\n",
    "    shuffle=True,\n",
    "    batch_size=args.batch_size,\n",
    "    pin_memory=False,\n",
    "    collate_fn=collate,\n",
    ")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    num_workers=args.num_workers,\n",
    "    shuffle=False,\n",
    "    batch_size=args.batch_size,\n",
    "    pin_memory=False,\n",
    "    collate_fn=collate,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    num_workers=args.num_workers,\n",
    "    shuffle=False,\n",
    "    batch_size=args.batch_size,\n",
    "    pin_memory=False,\n",
    "    collate_fn=collate,\n",
    ")\n",
    "\n",
    "args.total_steps = int(math.ceil(len(train_loader.dataset) / args.batch_size)) * (\n",
    "        args.n_epochs\n",
    "    )\n",
    "args.warmup_steps = args.total_steps // 10 # 이 변수가 무엇인지 잘 모르겠음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 이 모델은 베이스라인 LSTM 모델\n",
    "# class MODEL(nn.Module):\n",
    "#     def __init__(self, args):\n",
    "#         super(MODEL, self).__init__()\n",
    "#         self.args = args\n",
    "\n",
    "#         self.hidden_dim = self.args.hidden_dim\n",
    "#         self.n_layers = self.args.n_layers\n",
    "\n",
    "#         # Embedding\n",
    "\n",
    "#         self.embedding_item = nn.Embedding(self.args.n_item + 1, self.hidden_dim // 3)#self.hidden_dim // 4)\n",
    "#         self.embedding_test = nn.Embedding(self.args.n_test + 1, self.hidden_dim // 3)#self.hidden_dim // 4)\n",
    "#         self.embedding_tag = nn.Embedding(self.args.n_tag + 1, self.hidden_dim // 3)#self.hidden_dim // 4)\n",
    "#         self.embedding_grade = nn.Embedding(self.args.n_grade + 1, 2) # self.hidden_dim // 4)\n",
    "\n",
    "#         self.embedding_interaction = nn.Embedding(3, self.hidden_dim // 3)\n",
    "\n",
    "#         # embedding combination projection\n",
    "#         self.comb_proj = nn.Linear((self.hidden_dim // 3) * 4+2, self.hidden_dim)\n",
    "\n",
    "#         self.lstm = nn.LSTM(\n",
    "#             self.hidden_dim, self.hidden_dim, self.n_layers, batch_first=True\n",
    "#         )\n",
    "\n",
    "#         # Fully connected layer\n",
    "#         self.fc = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "#     def forward(self, input):\n",
    "\n",
    "#         test, item, tag, grade, interaction, mask, _ = input\n",
    "\n",
    "#         batch_size = interaction.size(0)\n",
    "\n",
    "#         # Embedding\n",
    "#         embed_interaction = self.embedding_interaction(interaction)\n",
    "#         embed_item = self.embedding_item(item)\n",
    "#         embed_test = self.embedding_test(test)\n",
    "#         embed_tag = self.embedding_tag(tag)\n",
    "#         embed_grade = self.embedding_grade(grade)\n",
    "\n",
    "#         embed = torch.cat(\n",
    "#             [\n",
    "#                 embed_interaction,\n",
    "#                 embed_item,\n",
    "#                 embed_test,\n",
    "#                 embed_tag,\n",
    "#                 embed_grade,\n",
    "#             ],\n",
    "#             2,\n",
    "#         )\n",
    "\n",
    "#         X = self.comb_proj(embed)\n",
    "\n",
    "#         # X : (batch * max_seq_len * hidden_dim)\n",
    "#         # out : (batch * max_seq_len * hidden_dim)\n",
    "#         out, _ = self.lstm(X)\n",
    "#         out = out.contiguous().view(batch_size, -1, self.hidden_dim)\n",
    "#         out = self.fc(out).view(batch_size, -1)\n",
    "#         return out[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 모델은 내가 실험하고 있는 모델.\n",
    "# 내(성연)가 슬랙에 올린 사진과 함께 보면 좋을 듯.\n",
    "\n",
    "try:\n",
    "    from transformers.modeling_bert import BertConfig, BertEncoder, BertModel\n",
    "except:\n",
    "    from transformers.models.bert.modeling_bert import (\n",
    "        BertConfig,\n",
    "        BertEncoder,\n",
    "        BertModel,\n",
    "    )\n",
    "\n",
    "# 하이퍼 파라미터 내가 실험하고 싶어서 여기서 바꿔줌.\n",
    "args.hidden_dim = 32\n",
    "args.lr = 0.001\n",
    "args.dropout = 0.2\n",
    "args.n_layers = 2\n",
    "args.n_heads = 2\n",
    "\n",
    "class MODEL(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(MODEL, self).__init__()\n",
    "        self.args = args\n",
    "\n",
    "        self.hidden_dim = self.args.hidden_dim\n",
    "        self.n_layers = self.args.n_layers\n",
    "        self.dropout = self.args.dropout\n",
    "\n",
    "        # Embedding (변수 unique 개수, 임베딩 벡터 크기)\n",
    "        # correct는 중요한 정보(해당 문제 맞았는지 틀렸는지)이기 때문에 임베딩 결과 크기를 크게 잡음.\n",
    "        # 임베딩 크기를 item은 종류가 많아 풍부한 표현을 위해 hidden_dim\n",
    "        # test와 tag는 이보다 떨어지기 때문에 hidden_dim // 2.\n",
    "        # grade는 몇개 없기 때문에(9개?) 고정된 임베딩 벡터 크기 값 3 사용\n",
    "        self.embedding_correct = nn.Embedding(3, self.hidden_dim) \n",
    "        self.embedding_item = nn.Embedding(self.args.n_item + 1, self.hidden_dim)\n",
    "        self.embedding_test = nn.Embedding(self.args.n_test + 1, self.hidden_dim // 2)\n",
    "        self.embedding_tag = nn.Embedding(self.args.n_tag + 1, self.hidden_dim // 2)\n",
    "        self.embedding_grade = nn.Embedding(self.args.n_grade + 1, 3) \n",
    "\n",
    "        # embedding combination projection\n",
    "\n",
    "        # 시험 문제 정보 만들기(임베딩 벡터 concat.)\n",
    "        # 임베딩 벡터 concat 크기는 self.hidden_dim + (self.hidden_dim // 2) * 2 + 3.\n",
    "        self.comb_proj = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim * 2 + 3, self.hidden_dim) ,  \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(p=self.dropout), \n",
    "        )\n",
    "\n",
    "        # BERT(트랜스포머) 모델 사용.\n",
    "        # max_seq_len - '1' 인 이유? \n",
    "        # => 현재 푼 문제가 맞았는지 틀렸는지 판단. 모델에서는 현재 푼 문제를 고려하고 싶지 않았음.\n",
    "        # => 여기서 추출하고 싶었던 것은 유저의 과거 문제 풀이 기록을 통해 어느정도 실력을 가지고 있는지 뽑아내고싶음.\n",
    "        self.config = BertConfig(\n",
    "            3,  # not used\n",
    "            hidden_size=self.hidden_dim, # 입력/출력 히든 레이어 길이.\n",
    "            num_hidden_layers=self.args.n_layers, # n_layers 수(1) head 당 몇 번 반복?\n",
    "            num_attention_heads=self.args.n_heads, # head 수(1) 같은 행위 몇 번 반복?\n",
    "            max_position_embeddings=self.args.max_seq_len - 1, # 최대 문제 길이 개수.\n",
    "        )\n",
    "\n",
    "        # Bert Layer\n",
    "        self.encoder = BertModel(self.config)\n",
    "        \n",
    "\n",
    "        # 혹시나 LSTM을 사용하고 싶다면 이거 사용하면 됨.\n",
    "        # self.lstm = nn.LSTM(\n",
    "        #     self.hidden_dim, self.hidden_dim, self.n_layers, batch_first=True\n",
    "        # )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(self.hidden_dim, 1) # self.hidden_dim\n",
    "\n",
    "    def forward(self, input):\n",
    "        # test, item, tag, grade, correct, mask, ansewer.\n",
    "        test, item, tag, grade, correct, mask, _ = input\n",
    "\n",
    "        batch_size = test.size(0)\n",
    "\n",
    "        # Embedding\n",
    "        embed_correct = self.embedding_correct(correct[:,:-1]) # 마지막 값은 현재 문제 정보. 들어가면 안됨.\n",
    "        embed_item = self.embedding_item(item)\n",
    "        embed_test = self.embedding_test(test)\n",
    "        embed_tag = self.embedding_tag(tag)\n",
    "        embed_grade = self.embedding_grade(grade)\n",
    "\n",
    "\n",
    "        embed = torch.cat(\n",
    "            [\n",
    "                embed_item,\n",
    "                embed_test,\n",
    "                embed_tag,\n",
    "                embed_grade\n",
    "            ],\n",
    "            2,\n",
    "        )\n",
    "\n",
    "        embed = self.comb_proj(embed)\n",
    "\n",
    "        # 문제 정보(현재 문제 제외)와 맞췄는지 정보를 결합해 줌.\n",
    "        # 해당 문제를 맞췄는지, 틀렸는지 정보를 통해 해당 학생의 실력을 embed_before에 전달.\n",
    "        embed_before = torch.mul(embed[:,:-1,:], embed_correct)\n",
    "\n",
    "        # mul이 아니고 concat 한 후 linear layer 통과 시킬수도. 아래 두 코드. \n",
    "        # embed_cat = torch.cat([embed[:,:-1,:], embed_correct], 2)\n",
    "        # embed_before = self.label_proj(embed_crt) # B X (S-1) X H\n",
    "\n",
    "        # (max_seq_len -1) 만큼 학생 실력 정보를 트랜스포머 인코딩 부분을 통해 상호작용 구하기.\n",
    "        # 이건 맞췄고, 저 실력은 부족하고.. 를 통해 학생의 최종 능력치(out) 구하기.\n",
    "        encoded_layers = self.encoder(inputs_embeds=embed_before, attention_mask=mask[:,:-1])\n",
    "        out = encoded_layers[0] # (batch * max_seq_len * hidden_dim)\n",
    "\n",
    "        # BERT 대신 lstm 사용시 코드.\n",
    "        # out, _ = self.lstm(embed_before)\n",
    "\n",
    "        # 학생의 최종 능력치와 현재 문제에 대한 정보 벡터 곱하기.\n",
    "        out = torch.mul(out[:,-1,:], embed[:,-1,:])\n",
    "        # 최종 분류(맞췄냐? 틀렸냐?)을 위한 선형 레이어.\n",
    "        out = self.fc(out).view(batch_size, -1) # (batch_size, hidden_dim)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "model = MODEL(args)\n",
    "model.to(args.device)\n",
    "optimizer = Adam(model.parameters(), lr=args.lr, weight_decay=0.01)\n",
    "scheduler = ReduceLROnPlateau(\n",
    "            optimizer, patience=10, factor=0.5, mode=\"max\", verbose=True\n",
    ")\n",
    "loss_f = nn.BCEWithLogitsLoss(reduction=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch, swichs = True): # True(내가만든모델), False(베이스라인 LSTM) \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        batch : tuple(5, test/question/tag/correct/mask)\n",
    "        tuple(test 등) : (batch_size : 64, max_seq_len : 20)\n",
    "    Returns:\n",
    "        tuple(test, question, tag, correct, mask, interaction)\n",
    "        tuple(test 등) : (batch_size : 64, max_seq_len : 20), masking을 잘 해줌.\n",
    "    \"\"\"\n",
    "    test, item, tag, grade, correct, mask = batch\n",
    "    #test, question, tag, correct, mask = batch\n",
    "\n",
    "    label = correct[:, -1] # [batch_size(64)]\n",
    "\n",
    "    # change to float\n",
    "    mask = mask.float()\n",
    "    correct = correct.float()\n",
    "    \n",
    "    #  test_id, question_id, tag\n",
    "    test = ((test + 1) * mask).int()\n",
    "    item = ((item + 1) * mask).int()\n",
    "    tag = ((tag + 1) * mask).int()\n",
    "    grade = ((grade) * mask).int() # grade는 0이 없음. 1부터 시작.\n",
    "\n",
    "\n",
    "    if swichs == False:\n",
    "        # interaction을 임시적으로 correct를 한칸 우측으로 이동한 것으로 사용\n",
    "        interaction = correct + 1  # 패딩을 위해 correct값에 1을 더해준다.\n",
    "        interaction = interaction.roll(shifts=1, dims=1)\n",
    "        interaction_mask = mask.roll(shifts=1, dims=1)\n",
    "        interaction_mask[:, 0] = 0\n",
    "        interaction = (interaction * interaction_mask).to(torch.int64)\n",
    "\n",
    "        return (test, item, tag, grade, interaction, mask, label)\n",
    "\n",
    "    else:\n",
    "        # 베이스라인 대비 추가. correct 0 : 기록x, 1 : 못푼문제, 2 : 푼문제, 3 : 풀었는지 맞출문제.\n",
    "        correct = ((correct + 1) * mask).int()\n",
    "        correct[:, -1] = 3\n",
    "\n",
    "        return (test, item, tag, grade, correct, mask, label)\n",
    "\n",
    "def get_metric(targets, preds):\n",
    "    auc = roc_auc_score(targets, preds)\n",
    "    acc = accuracy_score(targets, np.where(preds >= 0.5, 1, 0))\n",
    "\n",
    "    return auc, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train(train_loader, model, optimizer, scheduler, args):\n",
    "    model.train()\n",
    "\n",
    "    total_preds = []\n",
    "    total_targets = []\n",
    "    losses = []\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        # process_batch return 값 : tuple. (요소 별 튜플)\n",
    "        # input : list(columns(6) * batch_size * max_seq_len)\n",
    "        input = list(map(lambda t: t.to(args.device), process_batch(batch)))\n",
    "        #print(input[-3][:,-1])\n",
    "        #break\n",
    "\n",
    "        preds = (model(input))[:,-1]\n",
    "        targets = input[-1] \n",
    "\n",
    "        #preds = (model(input))[:,-1].squeeze()\n",
    "        #targets = input[-1].squeeze() \n",
    "\n",
    "        loss = loss_f(preds, targets)\n",
    "        loss = torch.mean(loss)\n",
    "        loss.backward()\n",
    "        # clip_grad_norm_ : 그래디언트(기울기) 소실/폭파 문제 개선을 위해 사용했데.\n",
    "        # 역전파 기울기 최댓 임계값 설정해서 이 값 안 넘게 하는 듯.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip_grad)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if step % args.log_steps == 0:\n",
    "            print(f\"Training steps: {step} Loss: {str(loss.item())}\")\n",
    "\n",
    "        # predictions\n",
    "        #preds = preds[:, -1]\n",
    "        #targets = targets[:, -1]\n",
    "\n",
    "        total_preds.append(preds.detach())\n",
    "        total_targets.append(targets.detach())\n",
    "        \n",
    "        losses.append(loss)\n",
    "\n",
    "    total_preds = torch.concat(total_preds).cpu().numpy()\n",
    "    # 밑 1줄 베이스라인 대비 추가 / pred 값에 시그모이드 함수 적용. \n",
    "    total_preds = 1 / (1 + np.exp(-total_preds))\n",
    "    total_targets = torch.concat(total_targets).cpu().numpy()\n",
    "\n",
    "    # Train AUC / ACC\n",
    "    auc, acc = get_metric(total_targets, total_preds)\n",
    "    loss_avg = sum(losses) / len(losses)\n",
    "    print(f\"TRAIN AUC : {auc} ACC : {acc}\")\n",
    "    #breakpoint()\n",
    "    return auc, acc, loss_avg\n",
    "\n",
    "\n",
    "def validate(valid_loader, model, args):\n",
    "    model.eval()\n",
    "\n",
    "    total_preds = []\n",
    "    total_targets = []\n",
    "    for step, batch in enumerate(valid_loader):\n",
    "        input = list(map(lambda t: t.to(args.device), process_batch(batch)))\n",
    "\n",
    "        preds = (model(input))[:,-1]#.squeeze()\n",
    "        targets = input[-1]\n",
    "\n",
    "        #preds = (model(input))[:,-1].squeeze()\n",
    "        #targets = input[-1].squeeze() # correct, [:,-1].unsqueeze(1)\n",
    "\n",
    "\n",
    "        # predictions\n",
    "        #preds = preds[:, -1]\n",
    "        #targets = targets[:, -1]\n",
    "\n",
    "        total_preds.append(preds.detach())\n",
    "        total_targets.append(targets.detach())\n",
    "\n",
    "    total_preds = torch.concat(total_preds).cpu().numpy()\n",
    "    # 밑 1줄 베이스라인 대비 추가 / pred 값에 시그모이드 함수 적용. \n",
    "    total_preds = 1 / (1 + np.exp(-total_preds))\n",
    "    total_targets = torch.concat(total_targets).cpu().numpy()\n",
    "\n",
    "    # Train AUC / ACC\n",
    "    auc, acc = get_metric(total_targets, total_preds)\n",
    "\n",
    "    print(f\"VALID AUC : {auc} ACC : {acc}\\n\")\n",
    "\n",
    "    return auc, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 모델이 학습이 잘 되는지, 안되고 있는건 아닌지 확인을 위한 코드임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training: Epoch 1\n",
      "Training steps: 0 Loss: 0.7157275676727295\n",
      "Training steps: 200 Loss: 0.6145651340484619\n",
      "Training steps: 400 Loss: 0.5741449594497681\n",
      "Training steps: 600 Loss: 0.5806995630264282\n",
      "Training steps: 800 Loss: 0.551110029220581\n",
      "Training steps: 1000 Loss: 0.5278575420379639\n",
      "Training steps: 1200 Loss: 0.6112567186355591\n",
      "Training steps: 1400 Loss: 0.5412204265594482\n",
      "Training steps: 1600 Loss: 0.5908793210983276\n",
      "Training steps: 1800 Loss: 0.56856369972229\n",
      "Training steps: 2000 Loss: 0.6467379331588745\n",
      "TRAIN AUC : 0.7153236816968384 ACC : 0.6996266933497537\n",
      "VALID AUC : 0.7447102834005279 ACC : 0.7155029093931837\n",
      "\n",
      "Start Training: Epoch 2\n",
      "Training steps: 0 Loss: 0.5916007161140442\n",
      "Training steps: 200 Loss: 0.6208318471908569\n",
      "Training steps: 400 Loss: 0.6226291656494141\n",
      "Training steps: 600 Loss: 0.5577549934387207\n",
      "Training steps: 800 Loss: 0.5783360004425049\n",
      "Training steps: 1000 Loss: 0.578928530216217\n",
      "Training steps: 1200 Loss: 0.5688868165016174\n",
      "Training steps: 1400 Loss: 0.560645580291748\n",
      "Training steps: 1600 Loss: 0.5703347325325012\n",
      "Training steps: 1800 Loss: 0.5460844039916992\n",
      "Training steps: 2000 Loss: 0.5907714366912842\n",
      "TRAIN AUC : 0.7180198797008784 ACC : 0.7032019704433498\n",
      "VALID AUC : 0.748184709135158 ACC : 0.722291493488501\n",
      "\n",
      "Start Training: Epoch 3\n",
      "Training steps: 0 Loss: 0.5633293390274048\n",
      "Training steps: 200 Loss: 0.5682305693626404\n",
      "Training steps: 400 Loss: 0.6397429704666138\n",
      "Training steps: 600 Loss: 0.6387612223625183\n",
      "Training steps: 800 Loss: 0.6126053333282471\n",
      "Training steps: 1000 Loss: 0.6314501762390137\n",
      "Training steps: 1200 Loss: 0.6062917113304138\n",
      "Training steps: 1400 Loss: 0.6029505729675293\n",
      "Training steps: 1600 Loss: 0.590116560459137\n",
      "Training steps: 1800 Loss: 0.580750048160553\n",
      "Training steps: 2000 Loss: 0.5778769254684448\n",
      "TRAIN AUC : 0.720411997384921 ACC : 0.7061037561576354\n",
      "VALID AUC : 0.748464639038026 ACC : 0.7253740648379052\n",
      "\n",
      "Start Training: Epoch 4\n",
      "Training steps: 0 Loss: 0.5225256681442261\n",
      "Training steps: 200 Loss: 0.6063414812088013\n",
      "Training steps: 400 Loss: 0.6585636734962463\n",
      "Training steps: 600 Loss: 0.5526719093322754\n",
      "Training steps: 800 Loss: 0.5381487607955933\n",
      "Training steps: 1000 Loss: 0.5492016673088074\n",
      "Training steps: 1200 Loss: 0.5785024762153625\n",
      "Training steps: 1400 Loss: 0.5561977028846741\n",
      "Training steps: 1600 Loss: 0.5996750593185425\n",
      "Training steps: 1800 Loss: 0.554753303527832\n",
      "Training steps: 2000 Loss: 0.6272371411323547\n",
      "TRAIN AUC : 0.7191889803808091 ACC : 0.7058151169950739\n",
      "VALID AUC : 0.7478861803290133 ACC : 0.723954003879191\n",
      "\n",
      "Start Training: Epoch 5\n",
      "Training steps: 0 Loss: 0.6114252209663391\n",
      "Training steps: 200 Loss: 0.5373009443283081\n",
      "Training steps: 400 Loss: 0.5844120383262634\n",
      "Training steps: 600 Loss: 0.5670821666717529\n",
      "Training steps: 800 Loss: 0.6234097480773926\n",
      "Training steps: 1000 Loss: 0.5515853762626648\n",
      "Training steps: 1200 Loss: 0.6397019624710083\n",
      "Training steps: 1400 Loss: 0.6099379658699036\n",
      "Training steps: 1600 Loss: 0.5816045999526978\n",
      "Training steps: 1800 Loss: 0.5274620056152344\n",
      "Training steps: 2000 Loss: 0.6027240753173828\n",
      "TRAIN AUC : 0.7195457443529167 ACC : 0.7064693657635468\n",
      "VALID AUC : 0.7480800887142501 ACC : 0.7220490440565254\n",
      "\n",
      "Start Training: Epoch 6\n",
      "Training steps: 0 Loss: 0.530150294303894\n",
      "Training steps: 200 Loss: 0.5776861310005188\n",
      "Training steps: 400 Loss: 0.6462145447731018\n",
      "Training steps: 600 Loss: 0.5872782468795776\n",
      "Training steps: 800 Loss: 0.5509272813796997\n",
      "Training steps: 1000 Loss: 0.5829131603240967\n",
      "Training steps: 1200 Loss: 0.577598512172699\n",
      "Training steps: 1400 Loss: 0.5969431400299072\n",
      "Training steps: 1600 Loss: 0.5830134153366089\n",
      "Training steps: 1800 Loss: 0.5716131925582886\n",
      "Training steps: 2000 Loss: 0.5743163824081421\n",
      "TRAIN AUC : 0.7208975974416894 ACC : 0.7056149938423645\n",
      "VALID AUC : 0.746800356461935 ACC : 0.7149833748960931\n",
      "\n",
      "Start Training: Epoch 7\n",
      "Training steps: 0 Loss: 0.5691567659378052\n",
      "Training steps: 200 Loss: 0.5678988695144653\n",
      "Training steps: 400 Loss: 0.5881609916687012\n",
      "Training steps: 600 Loss: 0.5861179828643799\n",
      "Training steps: 800 Loss: 0.6110448837280273\n",
      "Training steps: 1000 Loss: 0.601847767829895\n",
      "Training steps: 1200 Loss: 0.6494024991989136\n",
      "Training steps: 1400 Loss: 0.5835357904434204\n",
      "Training steps: 1600 Loss: 0.6433260440826416\n",
      "Training steps: 1800 Loss: 0.5387496948242188\n",
      "Training steps: 2000 Loss: 0.5813511610031128\n",
      "TRAIN AUC : 0.7201111666425097 ACC : 0.7065078509852217\n",
      "VALID AUC : 0.747854070445665 ACC : 0.7228110279855916\n",
      "\n",
      "Start Training: Epoch 8\n",
      "Training steps: 0 Loss: 0.581876277923584\n",
      "Training steps: 200 Loss: 0.5949169397354126\n",
      "Training steps: 400 Loss: 0.6142104864120483\n",
      "Training steps: 600 Loss: 0.6939556002616882\n",
      "Training steps: 800 Loss: 0.6426961421966553\n",
      "Training steps: 1000 Loss: 0.5561614632606506\n",
      "Training steps: 1200 Loss: 0.5804532766342163\n",
      "Training steps: 1400 Loss: 0.5831663608551025\n",
      "Training steps: 1600 Loss: 0.58582603931427\n",
      "Training steps: 1800 Loss: 0.610267698764801\n",
      "Training steps: 2000 Loss: 0.6022768020629883\n",
      "TRAIN AUC : 0.7204051126956987 ACC : 0.7076970443349754\n",
      "VALID AUC : 0.7489969782671446 ACC : 0.7195552784704904\n",
      "\n",
      "Start Training: Epoch 9\n",
      "Training steps: 0 Loss: 0.5596020221710205\n",
      "Training steps: 200 Loss: 0.5572100877761841\n",
      "Training steps: 400 Loss: 0.563450813293457\n",
      "Training steps: 600 Loss: 0.6054325699806213\n",
      "Training steps: 800 Loss: 0.5527550578117371\n",
      "Training steps: 1000 Loss: 0.6027626395225525\n",
      "Training steps: 1200 Loss: 0.6415579319000244\n",
      "Training steps: 1400 Loss: 0.5877997875213623\n",
      "Training steps: 1600 Loss: 0.538394570350647\n",
      "Training steps: 1800 Loss: 0.5280697345733643\n",
      "Training steps: 2000 Loss: 0.64747154712677\n",
      "TRAIN AUC : 0.7217453857492653 ACC : 0.7088516009852217\n",
      "VALID AUC : 0.7487060838897694 ACC : 0.7229149348850097\n",
      "\n",
      "Start Training: Epoch 10\n",
      "Training steps: 0 Loss: 0.5369645357131958\n",
      "Training steps: 200 Loss: 0.6187816858291626\n",
      "Training steps: 400 Loss: 0.5674288868904114\n",
      "Training steps: 600 Loss: 0.5978673100471497\n",
      "Training steps: 800 Loss: 0.597626805305481\n",
      "Training steps: 1000 Loss: 0.5533150434494019\n",
      "Training steps: 1200 Loss: 0.6272436380386353\n",
      "Training steps: 1400 Loss: 0.5957678556442261\n",
      "Training steps: 1600 Loss: 0.5507209897041321\n",
      "Training steps: 1800 Loss: 0.5515186190605164\n",
      "Training steps: 2000 Loss: 0.5765607357025146\n",
      "TRAIN AUC : 0.7226111108792669 ACC : 0.7092595443349754\n",
      "VALID AUC : 0.7481496151848499 ACC : 0.7242310889443059\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.n_epochs): # 2\n",
    "\n",
    "    print(f\"Start Training: Epoch {epoch + 1}\")\n",
    "\n",
    "    ### TRAIN\n",
    "    train_auc, train_acc, train_loss = _train(\n",
    "        train_loader, model, optimizer, scheduler, args\n",
    "    )\n",
    "    auc, acc = validate(valid_loader, model, args)\n",
    "\n",
    "    scheduler.step(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6818,  1.3192, -0.8071,  1.3209,  0.9297,  0.6129,  1.1596, -0.6660,\n",
       "         0.9773, -0.7839,  0.9905,  1.3501,  0.1142,  0.6664,  0.2785,  1.1624,\n",
       "         1.2937,  1.2306,  0.7446, -0.6876, -0.8383, -0.8069,  1.0830,  1.3812,\n",
       "         0.4079,  0.7446,  0.7422,  1.2916, -0.6315, -0.3606, -0.0268,  1.2981,\n",
       "         1.3370, -0.4256,  0.0062,  0.3117,  1.3066,  1.1858,  1.3053,  1.1688,\n",
       "        -0.3958,  0.7714, -0.8350,  1.4001, -0.0113,  1.2799,  0.4233,  0.1882,\n",
       "        -0.3084, -0.3155,  0.5510,  0.8142,  1.1236,  1.2021,  1.3248, -0.3239,\n",
       "         0.8269, -0.6694,  1.3602,  1.3177,  1.1615,  0.8308,  1.0613,  0.8455,\n",
       "         0.1342, -0.5653,  1.1031,  0.6791,  1.3886, -0.6648,  1.3089, -0.1950,\n",
       "        -0.1703,  1.3375,  0.9260,  1.1990,  1.3306,  1.3852,  1.3062, -0.8182,\n",
       "        -0.7366,  0.3571,  1.0242,  0.8608, -0.6943,  1.3802,  1.0823, -0.7591,\n",
       "        -0.6912,  0.2458, -0.4940, -0.6589,  1.2750,  0.2347,  1.3398,  1.3513,\n",
       "         1.2572,  0.8112,  0.7041,  1.2520,  1.1297, -0.6959, -0.6715,  1.1090,\n",
       "         1.2118, -0.0889, -0.3987,  0.9260, -0.8162,  0.9533,  1.2172,  1.2834,\n",
       "         1.0782,  1.3468,  0.4429,  0.1199, -0.1595, -0.3199, -0.8309,  1.1739,\n",
       "        -0.7190, -0.7351,  1.2771, -0.8342,  0.6769,  1.2352,  1.3034, -0.5872],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for step, batch in enumerate(train_loader):\n",
    "    # process_batch return 값 : tuple. (요소 별 튜플)\n",
    "    # input : list(columns(6) * batch_size * max_seq_len)\n",
    "    input = list(map(lambda t: t.to(args.device), process_batch(batch)))\n",
    "    #print(input[-3][:,-1])\n",
    "    #break\n",
    "\n",
    "    preds = (model(input))[:,-1]#.squeeze()\n",
    "    break\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MODEL(\n",
       "  (embedding_correct): Embedding(3, 16)\n",
       "  (embedding_item): Embedding(9455, 16)\n",
       "  (embedding_test): Embedding(1538, 8)\n",
       "  (embedding_tag): Embedding(913, 8)\n",
       "  (embedding_grade): Embedding(10, 3)\n",
       "  (comb_proj): Sequential(\n",
       "    (0): Linear(in_features=35, out_features=16, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(3, 16, padding_idx=0)\n",
       "      (position_embeddings): Embedding(19, 16)\n",
       "      (token_type_embeddings): Embedding(2, 16)\n",
       "      (LayerNorm): LayerNorm((16,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (key): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (value): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (LayerNorm): LayerNorm((16,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=16, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=16, bias=True)\n",
       "            (LayerNorm): LayerNorm((16,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 19, 32])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = model.encoder(inputs_embeds=torch.rand([2, 19, 32]).to(args.device), attention_mask=torch.ones([2,19]).to(args.device))\n",
    "t[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MODEL(\n",
       "  (embedding_correct): Embedding(3, 16)\n",
       "  (embedding_item): Embedding(9455, 16)\n",
       "  (embedding_test): Embedding(1538, 8)\n",
       "  (embedding_tag): Embedding(913, 8)\n",
       "  (embedding_grade): Embedding(10, 3)\n",
       "  (comb_proj): Sequential(\n",
       "    (0): Linear(in_features=35, out_features=16, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (lstm): LSTM(16, 16, num_layers=3, batch_first=True)\n",
       "  (fc): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기까지가 모델 테스트 공간."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 예측 코드.\n",
    "model.eval()\n",
    "\n",
    "total_preds = []\n",
    "\n",
    "for step, batch in enumerate(test_loader):\n",
    "    input = list(map(lambda t: t.to(args.device), process_batch(batch)))\n",
    "    #print(input[-3][:, -1])\n",
    "    #break\n",
    "    preds = (model(input))[:,-1]#.squeeze()\n",
    "\n",
    "    # predictions\n",
    "    preds = preds.cpu().detach().numpy()\n",
    "    preds = 1 / (1 + np.exp(-preds))\n",
    "    total_preds += list(preds)\n",
    "\n",
    "write_path = os.path.join('/opt/ml/input/code/dkt/output', \"ksy_lstm_submission.csv\")\n",
    "# if not os.path.exists(args.output_dir):\n",
    "#     os.makedirs(args.output_dir)\n",
    "with open(write_path, \"w\", encoding=\"utf8\") as w:\n",
    "    w.write(\"id,prediction\\n\")\n",
    "    for id, p in enumerate(total_preds):\n",
    "        w.write(\"{},{}\\n\".format(id, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('dkt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09d4b5de27b89318d6e35b29f9634657110a12e6b33fcea780feda7aad3e9c4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
